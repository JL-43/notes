---
tags:
  - Software Engineering
  - Informational
  - Data Engineering
  - Data
  - Change Data Capture
  - Transactional Data
---

# January 25, 2025

Yesterday, I did not learn from Udemy but I did learn something from Wauter.

> It is common practice for transactional systems that overwrite 
> their transactions to do a check on their IDs, 
> match it to the ones in the data warehouse, 
> and if the IDs are missing: mark for soft delete.

--

## Transaction Data Overwrite Management

Transactional systems that support overwrites or updates via backfilling, which introduces complexities in maintaining data consistency in the downstream data warehouse. The scenario is fairly common in systems with mutable records or late-arriving data.

### The Challenge:

- **Overwrite-prone data**: The source system updates existing transactions with new IDs and may alter details while keeping certain key attributes the same (e.g., start time, end time, department).
- **Tracking changes**: It’s essential to track both the old and new versions of the data in your data warehouse.
- **Soft delete for missing IDs**: If an event ID disappears (is no longer sent by the source system), it might indicate the event was canceled or superseded.

### Approach to Address the Issue:

1. **Use Change Data Capture (CDC)** to detect changes in the transactional system.
2. **Maintain versioning** of the data in your warehouse (e.g., SCD Type 2 for historical changes).
3. **Perform ID reconciliation**: Cross-reference incoming IDs with existing warehouse IDs to mark missing IDs for soft deletion.

### Data Management Solutions (Quick Reference)

*Here is a quick summary of the topics that are also discussed in depth below*

| Solution                        | Description                                                                 | Pros                                                                                                                                                      | Cons                                                                                                     | Example Scenario                                                                                                                                                                                                                  |
|---------------------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **SCD Type 2**                  | Store historical versions of transactions in the same table.                | - Full historical tracking  <br>- Easy to query historical changes.                                                                                       | - Can grow large quickly  <br>- Requires careful partitioning and indexing.                              | **Table Design:** Add an `is_active` and `valid_to` column. Update the existing record's `is_active` to `FALSE` and add a new row for each change with `is_active = TRUE`.                                                       |
| **Append-Only Model**           | Store every update as a new record, with no overwriting.                    | - Simplifies ingestion  <br>- No need to compare previous and current states.                                                                             | - Requires deduplication for queries  <br>- Storage can grow quickly.                                    | **Implementation:** For every new transaction or update, append the record with a `timestamp` column for tracking when it was ingested. Use aggregation for current state queries.                                               |
| **Current and Historical Tables** | Separate current transactions and historical versions into two tables.     | - Simplifies querying active transactions  <br>- Historical queries are isolated.                                                                         | - Adds complexity to ETL pipelines  <br>- Requires managing synchronization between tables.              | **ETL Logic:** Load new data into `current_table`. When an overwrite occurs, move the previous version to `historical_table` and update `current_table` with the new transaction.                                                |
| **Soft Deletes**                | Mark missing or replaced transactions as "deleted" instead of removing them.| - Retains data integrity  <br>- Easy to implement in most databases.                                                                                      | - Queries need filters to exclude deleted rows.                                                          | **Table Design:** Add a `deleted_flag` or `status` column. Set it to `TRUE` or "deleted" for missing or replaced records during ETL reconciliation.                                                                             |
| **Delta/Versioned Tables**      | Use versioning or time travel capabilities in modern data platforms.        | - Native support in platforms like Delta Lake, BigQuery, or Snowflake  <br>- Querying historical versions is seamless.                                    | - Vendor lock-in  <br>- Higher storage costs for keeping snapshots.                                      | **Implementation:** Enable Delta Lake time travel. Use timestamps or version numbers to query specific states of the data.                                                                                                     |
| **Change Data Capture (CDC)**   | Continuously capture changes from the transactional system.                 | - Real-time updates  <br>- Captures all changes without requiring full reprocessing.                                                                      | - Requires CDC tools and infrastructure  <br>- Not all systems support native CDC.                       | **Tools:** Use Debezium, Fivetran, or Azure Data Factory. Write changes to a staging table, then reconcile with your data warehouse.                                                                                            |
| **Event-Based Pipeline**        | Treat every transaction as an immutable event in an event log.              | - Immutable log ensures no data loss  <br>- Great for audit and compliance use cases.                                                                     | - Complex queries to retrieve current state  <br>- Requires tools like Kafka or Kinesis for ingestion.   | **Pipeline Example:** Use Kafka to ingest events into a message queue. Store each event with metadata (e.g., event type: "create," "update") in a data lake or warehouse.                                                       |
| **Incremental Loads with Reconciliation** | Compare incoming data with the existing data warehouse and update accordingly. | - Efficient for systems with frequent updates  <br>- Only processes changes.                                                                             | - ETL logic can get complex  <br>- Requires efficient reconciliation strategies.                         | **ETL Example:** Use a staging table to load incoming data. Compare the `event_id` against the warehouse table. Insert new records, update changed records, and flag missing records for soft delete.                           |
| **Snapshotting**                | Take periodic snapshots of the transactional data.                          | - Simplifies historical data capture  <br>- Useful for point-in-time analysis.                                                                            | - May miss interim changes between snapshots  <br>- Storage grows based on snapshot frequency.           | **Example:** Take daily or hourly snapshots of the source data. Store snapshots in a separate table or partition with a `snapshot_date` column for time-based querying.                                                         |

### Practical Keywords for Further Research:

- Late-arriving data handling in ETL pipelines
- Mutable transactional data in data warehouses
- Slowly Changing Dimensions (SCD Type 2) for overwrite-prone data
- Data reconciliation and soft deletes in CDC pipelines
- Versioning transactional data for backfilled events
- Tracking overwrites in Delta Lake or BigQuery
- Data deduplication and conflict resolution in transactional systems

### Resources for This Specific Scenario:

#### Understanding the Concepts

- **Kimball Group**: Managing Overwrite-Prone Data: Look for articles about CDC, SCDs, and late-arriving data.
- **Book**: The Data Warehouse Toolkit by Ralph Kimball – Great for learning how to design data warehouses for mutable and late-arriving transactional data.

#### CDC and ETL Best Practices

- **Databricks Change Data Capture Documentation**: Details how to use Delta Lake to track and reconcile changes from mutable systems.
- **Google Cloud's CDC Practices**: Explains how to implement robust CDC pipelines.

#### Handling Soft Deletes and Missing IDs

- **Delta Lake Upserts and Deletes**: Shows how to implement upserts and soft deletes in Delta Lake.
- **SQL Soft Deletes for Warehousing**: Practical implementation for soft deletes.

#### Design Patterns for Versioning and History

- **Snowflake Time Travel**: A feature to retain old versions of data for a certain period.
- **BigQuery Best Practices for Historical Data**: How to handle versioning for mutable data in BigQuery.

#### Implementation Examples

- **dbt Incremental Models with Mutable Data**: dbt Incremental Models Documentation.
- **Soft Deletes and SCD Type 2 in PySpark**: Look for Databricks' examples or Medium articles on PySpark SCD Type 2.

